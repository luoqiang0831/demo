今天做一个爬虫的时候 需要爬网站里面的文章保存为文本文件 然后在导入到系统中 
在做的过程中遇到了三个问题 
1、抓取的是GB2312的网页 所以需要转换下
2、获取文章内容时使用cheerio抓取 但是获取内容是需要 元素.html() 这个地方出现了乱码 true-html-escape 进行处理
3、保存到本地时能正常打开 内容也没乱码 可其他程序识别时读取的是乱码 需要使用iconv.encode(str,'GB2312')保存前做处理
  因为js只有一种字符串类型。这个类型是UCS-2/UTF-16的，只有用Blob或Buffer才能真正保存UTF-8编码的数据。通过网页爬虫读取到的GB2312数据
  应该直接转换为js内部字符串，然后利用fs的API保存时指定上utf-8编码。这样对应的模块在保存时进行转换。如果你想自己转换
  ，那么结果只能放在Buffer里，再写文件。

样例：
var escaper = require("true-html-escape");
 
escaper.escape("¤¥€");                                                  ///<= &curren;&yen;&euro; 
escaper.unescape("&lt;span&gt;&#29579;&#23612;&#29595;&lt;/span&gt;");  ///<= <span>王尼玛</span> 
escaper.unescape("&#12501;&#12521;&#12531;&#12489;&#12540;&#12523;");   ///<= フランドール 
escaper.unescape("(&#x256d;&#xffe3;3&#xffe3;)&#x256d;&#x2661;")         ///<= (╭￣3￣)╭♡ 

顺便贴出iconv转换参数:
Node.js Native encodings: utf8, ucs2 / utf16le, ascii, binary, base64, hex
Unicode: UTF-16BE, UTF-16 (with BOM)
Single-byte:
Windows codepages: 874, 1250-1258 (aliases: cpXXX, winXXX, windowsXXX)
ISO codepages: ISO-8859-1 - ISO-8859-16
IBM codepages: 437, 737, 775, 808, 850, 852, 855-858, 860-866, 869, 922, 1046, 1124, 1125, 1129, 1133, 1161-1163 (aliases cpXXX, ibmXXX)
Mac codepages: maccroatian, maccyrillic, macgreek, maciceland, macroman, macromania, macthai, macturkish, macukraine, maccenteuro, macintosh
KOI8 codepages: koi8-r, koi8-u, koi8-ru, koi8-t
Miscellaneous: armscii8, rk1048, tcvn, georgianacademy, georgianps, pt154, viscii, iso646cn, iso646jp, hproman8, tis620
Multi-byte:
Japanese: Shift_JIS, Windows-31j, Windows932, EUC-JP
Chinese: GB2312, GBK, GB18030, Windows936, EUC-CN
Korean: KS_C_5601, Windows949, EUC-KR
Taiwan/Hong Kong: Big5, Big5-HKSCS, Windows950

